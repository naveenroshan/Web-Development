<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8" />
  <title>Audio tutorial</title>
</head>

<body style="background-color: rgb(211, 211, 212); display: flex;">
  <div>
    <video id="video1" data-playbutton="btn1" src="video/video.mp4" controls="true"></video>
  </div>
  <div>
    <canvas id="c1" width="160" height="96"></canvas>
    <canvas id="c2" width="160" height="96"></canvas>
  </div>
  <br>
  <div style="display:flex;">
    <video id="video2" data-playbutton="btn2" src="video/video2.mp4" controls="true"></video>
    <br>
    <button id="startRecording" onclick="processor.startRecording()">
      Start Recording
    </button>
    <button id="stopRecording" onclick="processor.stopRecording()" disabled="disabled">
      Stop Recording
    </button>
    <button id="downloadRecording" onclick="processor.downloadRecording()">
      Download Recording
    </button>
    <a id="downloadButton" class="button">
      Download
    </a>
  </div>

  <script type="text/javascript">
    let processor = {
      //getting the video from video convas using mediastream
      videoNewMediaStreams: function () {
        if (this.newMediaStream === undefined || this.newMediaStream === null)
          this.newMediaStream = new MediaStream();

        console.log(this.newMediaStream.getVideoTracks().length);

        this.c1.captureStream(25).getVideoTracks().forEach(videoTrack => {
          this.newMediaStream.addTrack(videoTrack);
        });

        console.log(this.newMediaStream.getVideoTracks().length);
      },

      timerCallback: function () {
        if (this.video.paused || this.video.ended) {
          return;
        }
        this.computeFrame();
        let self = this;
        setTimeout(function () {
          self.timerCallback();
        }, 0);
      },

      doLoad: function () {
        this.video = document.getElementById("video1");
        this.c1 = document.getElementById("c1");
        this.ctx1 = this.c1.getContext("2d");
        let self = this;
        this.video.addEventListener("play", function (e) {
          self.width = self.video.videoWidth / 2;
          self.height = self.video.videoHeight / 2;
          self.timerCallback();
        }, false);
        this.recorder = null;
      },

      computeFrame: function () {
        this.ctx1.drawImage(this.video, 0, 0, this.width, this.height);
        return;
      },
      //getting the audio using audio context
      audioNewMediaStreams: function () {
        // for cross browser compatibility
        const AudioContext = window.AudioContext || window.webkitAudioContext;
        let audioContext = null;

        console.log("Enabled audio context")

        //get the audio element
        const audioElemnt = document.querySelector('#video2');

        //when the track finishes playing
        audioElemnt.addEventListener('ended', function () {
          playButton.dataset.playing = 'false';
        }, false);

        //audio context object
        audioContext = new AudioContext();

        //pass it into the audio context
        const track = audioContext.createMediaElementSource(audioElemnt);

        //modifying the sound
        const gainNode = audioContext.createGain();

        //the AudioContext Interface is used to create a new MediaStreamAudioDestinationNode object associated with a WebRTC MediaStream 
        //representing an audio stream, which may be stored in a local file
        let dest2 = audioContext.createMediaStreamDestination();
        track.connect(gainNode).connect(dest2);
        console.log(dest2);
        console.log(dest2.stream.getAudioTracks().length);
        //getting the stream audio and pushing to the newMediaStream
        dest2.stream.getAudioTracks().forEach((audioTrack) => {
          this.newMediaStream.addTrack(audioTrack);
        });

        console.log(this.newMediaStream.getAudioTracks().length);
        console.log(this.newMediaStream.getVideoTracks().length);
        console.log(this.newMediaStream);
      },

      startRecording: function () {

        this.videoNewMediaStreams();
        this.audioNewMediaStreams();
        console.log(this.newMediaStream);
        //passing the newwMediaStream into the Media Recorder
        this.recorder = new MediaRecorder(this.newMediaStream);

        //Creates an empty array, data, which will be used to hold the Blobs
        //of media data provided to our ondataavailable event handler.
        this.data = [];

        //Sets up the handler for the dataavailable event. 
        //The received event's data property is a Blob that contains the media data.
        //The event handler simply pushes the Blob onto the data array.
        this.recorder.ondataavailable = event => {
          this.data.push(event.data);
          console.log(" media recorder returned data @ " + new Date().toLocaleString());
        }
        this.recorder.start(5000);
        document.getElementById("startRecording").setAttribute("disabled", "disabled");
        document.getElementById("stopRecording").removeAttribute("disabled");
      },

      stopRecording: function () {
        this.recorder.stop();
        console.log(" media recorder stopped @ " + new Date().toLocaleString());
        console.log("number of chunks recorded = " + this.data.length);
        document.getElementById("startRecording").removeAttribute("disabled");
        document.getElementById("stopRecording").setAttribute("disabled", "disabled");
      },

      downloadRecording: function () {
        let recordedChunks = this.data;
        console.log("downloading " + recordedChunks);
        // The recording process's resolution handler receives as input an array of media data Blobs locally known as recordedChunks. 
        // The first thing we do is merge the chunks into a single Blob whose MIME type is "video/webm" by taking advantage of the fact that the Blob() 
        // constructor concatenates arrays of objects into one object.
        let recordedBlob = new Blob(recordedChunks, {
          type: "video/webm"
        });

        console.log("blob ceated from chunks", recordedBlob);
        //Then URL.createObjectURL() is used to create an URL that references the blob;
        //this is then made the value of the recorded video playback element's src attribute
        //(so that you can play the video from the blob)
        let recordedData = URL.createObjectURL(recordedBlob);
        let downloadButton = document.getElementById("downloadButton");
        //set the target of the download button's link.
        //Then the download button's download attribute is set. 
        downloadButton.href = recordedData;

        //So by setting the download link's download attribute to "RecordedVideo.webm", 
        // we tell the browser that clicking the button should download a file named "RecordedVideo.webm" 
        // whose contents are the recorded video.
        downloadButton.download = "recordeding.webm";
      }
    };

    document.addEventListener("DOMContentLoaded", () => {
      processor.doLoad();
    });
  </script>
</body>

</html>